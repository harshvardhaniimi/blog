---
title: Language Models and Multiverse
author: Harshvardhan
date: '2025-09-01'
slug: language-models-and-multiverse
categories:
  - blog
  - thoughts
tags: []
draft: no
---



<p>I was listening to a podcast by OpenAI where they tell us how they trained GPT-4.5, one of the largest models they have today.
GPT-4.5 shows intelligence in unexpected ways, demonstrating common sense like other models totally miss.
I haven’t used it much myself so cannot comment on that, but at <a href="https://youtu.be/6nJZopACRuQ?t=2318">38:38</a> in the video, Sam Altman asks Daniel Selsam “why does supervised learning work?”.
Without skipping a beat, he replies “compression”.
Then he explains that “the ideal intelligence is Solomonoff Induction”.
Unfamiliar with this term, I jumped on a conversation with GPT-4.5 and along the way, I learnt several interesting things that I want to share with you all.</p>
<div id="solomonoff-induction" class="section level2">
<h2>Solomonoff Induction</h2>
<p>Imagine that you are given the following set of numbers and you have to predict the next one:</p>
<blockquote>
<p>2, 4, 6, 8, ___</p>
</blockquote>
<p>Did you guess 10?
Why?
Probably because that’s the simplest pattern — “add two each time”.
But consider this alternative explanation — “Add two every time until you reach 8, then suddenly switch to adding five”.
Then, the next number would be 13.</p>
<p>Both explanations match the observed data.
Yet, intuitively, you’d bet on 10 because it’s simpler.
<strong>But why exactly does simplicity feel right?</strong></p>
<p>Simplicity feels right because of <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam’s Razor</a>: “Among competing hypotheses, choose the simplest one”.</p>
<p>Then, in 1960s, Ray Solomonoff had a brilliant idea.
He decided to represent each possible explanation (or hypothesis) as a computer program.
The shorter the program length (in bits or complexity), the simpler the hypothesis.</p>
<p>For example, there are several computer programs to decide the next number in our series <code>2, 4, 6, 8, ___</code>:</p>
<table>
<colgroup>
<col width="45%" />
<col width="25%" />
<col width="29%" />
</colgroup>
<thead>
<tr class="header">
<th>Program</th>
<th>Length (in bits)</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Print all even numbers</td>
<td>Short</td>
<td>Simple, general</td>
</tr>
<tr class="even">
<td>Print 2, 4, 6, 8, then print 13, 18, … (add 5 to last number)</td>
<td>Medium</td>
<td>Complicated, special-case explanation</td>
</tr>
<tr class="odd">
<td>Print numbers 2, 4, 6, 8, followed by random unpredictable numbers</td>
<td>Very Long</td>
<td>Highly complex, arbitrary, no clear pattern</td>
</tr>
</tbody>
</table>
<p>(While <a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogorov complexity</a>—the length of the shortest program that generates your data—is a powerful theoretical idea, it’s actually uncomputable in practice, so we use description length as an ideal guide for simplicity.)</p>
<p>The shortest program often has the simplest interpretation.
<strong>Supervised learning algorithms are attempts to compress all available information in data useful for decision making into simple binary.</strong> Solomonoff said that to predict the next event, we should:</p>
<ol style="list-style-type: decimal">
<li>Consider every possible program that could produce the observed data.</li>
<li>Assign probabilities based on simplicity:
<ol style="list-style-type: decimal">
<li>Shorter program → Higher probability</li>
<li>Longer program → Lower probability</li>
<li><span class="math inline">\(\text{Probability} \propto 2^{-\text{program length}}\)</span></li>
</ol></li>
<li>Predict the next event by taking a weighted average of the predictions from <strong>all programs</strong>.</li>
</ol>
<p>In other words, we can imagine an infinite “multiverse” of programs generating your data.
We don’t just pick one; we average over them all, weighing each according to simplicity.</p>
<p>Even though practically it is impossible to use, it guides us to understand that simpler algorithms that compress data better are almost always better representations of data.
Additionally, since Solomonoff induction relies on an uncomputable prior, real-world machine learning uses practical substitutes like Minimum Description Length (MDL), Bayesian model averaging, or ensembling to approximate the idea of weighing simpler explanations more heavily.</p>
</div>
<div id="multiverse" class="section level2">
<h2>Multiverse?</h2>
<p>Now, while exploring Solomonoff induction, I stumbled upon its intriguing connection with another concept: the multiverse.
Physicist Max Tegmark proposed <a href="https://space.mit.edu/home/tegmark/PDF/multiverse_sciam.pdf">four levels of multiverse</a>, each level capturing a different kind of parallel universe.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>The first level Tegmark describes includes regions beyond our observable cosmic horizon.
Essentially, these universes are extensions of our own, governed by the same physical laws but possibly differing in their initial conditions.</p>
<p>The second level imagines universes arising from something called eternal inflation (recall that because of cosmic inflation our universe is constantly expanding), each potentially having different physical constants and laws of nature.
Solomonoff induction would, in theory, incorporate these universes too, evaluating them based on how simple and computable their fundamental rules are.</p>
<p>Shorter, simpler descriptions (or programs) of universes would be considered more probable than complicated, special-case scenarios.</p>
<p>The third level relates directly to quantum mechanics, specifically the many-worlds interpretation.
Here, every quantum event creates branching universes, each representing different outcomes.
This concept resonates strongly with Solomonoff induction’s approach, where each potential future event is considered simultaneously, weighted by simplicity and computability.
Each “branch” is akin to a different program output, contributing to the overall prediction.</p>
<p>Finally, Tegmark’s fourth level—the “ultimate ensemble”—is the broadest and most abstract.
It suggests that every mathematically consistent universe that can exist does exist.
This level again matches perfectly with Solomonoff induction.
Since Solomonoff induction evaluates every conceivable computable hypothesis, it inherently encompasses this idea, assigning probabilities to these universes based purely on the elegance and simplicity of their mathematical description.</p>
<p>It’s worth noting that in quantum mechanics, the many-worlds interpretation assigns probabilities to outcomes using the <a href="https://en.wikipedia.org/wiki/Born_rule">Born rule</a>, not by simplicity—so the analogy with Solomonoff is more metaphorical than literal.
Also, eternal inflation refers to the ongoing creation of new “pocket universes” with different constants, while the current expansion of our universe is driven by dark energy.</p>
</div>
<div id="okay-but-how-is-this-related-to-language-models" class="section level2">
<h2>Okay, but how is this related to language models?</h2>
<p>Bringing it back to language models: when models like GPT-4.5 predict text, they’re essentially compressing huge amounts of information into compact representations—similar to the spirit of Solomonoff induction.
Training methods like cross-entropy loss connect to Minimum Description Length, rewarding models that capture patterns efficiently.</p>
<p>When sampling outputs, techniques like temperature scaling and nucleus sampling let us explore a range of possible continuations, almost like averaging predictions from multiple “programs” or hypotheses.
In practice, ensembling and mixture-of-experts architectures mimic the idea of averaging over many models, echoing the theoretical blend of simplicity and diversity that Solomonoff induction imagines.</p>
</div>
<div id="is-multiverse-real" class="section level2">
<h2>Is Multiverse Real?</h2>
<p>We don’t know.
But as a fan of “Everything Everywhere All at Once”, I would totally say “Yes!”.</p>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>See “Physics in the multiverse: an introductory review” by Aurélien Barrau for brief introduction on multiverse theory in Physics.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
