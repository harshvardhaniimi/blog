---
title: Joint Probability-based Dissimilarity Measure for Discrete Variables
subtitle: A Simple Method to Calculate Distance between Discrete Variable
summary: A Simple Method to Calculate Distance between Discrete Variable
author: Harshvardhan
date: '2022-05-10'
slug: joint-probability-based-dissimilarity-measure-for-discrete-variables
categories:
  - statistics
  - R
  - ML
tags: []
links:
- icon: book
  icon_pack: fas
  name: Report
  url: /docs/joint_probability_based_distance.pdf
---



<p><strong>For details, read the complete report. This project was part of <a href="http://web.utk.edu/~wzhou4/">Prof Wenjun Zhou</a>’s Machine Learning class at University of Tennessee.</strong></p>
<div id="what-is-it" class="section level1">
<h1>What is it?</h1>
<p>Most clustering algorithms work for numerical variables where the variables are assumed to be continuous and random. In this short monograph, I proposed a probability-based distance measure for computing dissimilarity between observations for discrete variables thought to be randomly distributed. As their probabilities are derived empirically, there is no underlying assumption on their distribution.</p>
</div>
<div id="how-do-i-do-it" class="section level1">
<h1>How do I do it?</h1>
<p>Consider two discrete random variables <span class="math inline">\(X_1\)</span> with <span class="math inline">\(u\)</span> different classes and <span class="math inline">\(X_2\)</span> with <span class="math inline">\(v\)</span> different classes. Let <span class="math inline">\(\{c_{11}, c_{12}, \dots, c_{1u} \}\)</span> be the set of different classes of <span class="math inline">\(X_1\)</span>. Similarly, let <span class="math inline">\(\{ c_{21}, c_{22}, \dots, c_{2v}\}\)</span> be the set of different classes of <span class="math inline">\(X_2\)</span>. The empirical probability of event <span class="math inline">\(X_1 = c_i\)</span> is <span class="math inline">\(\frac{m}{n}\)</span>, where <span class="math inline">\(m\)</span> is the frequency of <span class="math inline">\(c_i\)</span> observed in <span class="math inline">\(X_1\)</span> and <span class="math inline">\(n\)</span> is the total number of observations.</p>
<p>Assuming that the sample is representative of the population, we can calculate the empirical probability of each class for each variable. Once we have those probabilities, we can calculate the joint probability for an observation that I call “score”. This score is a number between 0 and 1.</p>
<div id="interpretation" class="section level2">
<h2>Interpretation</h2>
<p>The score of zero is asymptotically possible but impossible in real-world analysis. If the researcher assumes no prior knowledge about the variable, only the existing classes observed in the data can be used as a possible class. In that case, the score cannot be zero for any observation. However, if the researcher assigns a non-zero probability to a class that wasn’t observed in real data, we can have zero probability for some classes. A score of one is possible only when all observations are precisely the same.</p>
<p>In most general cases, the value for each observation would lie between zero and one. The closer the values are to each other, the closer they are to each other (although this is not guaranteed, as we will see in the following example.)</p>
</div>
</div>
<div id="pros-and-cons" class="section level1">
<h1>Pros and Cons</h1>
<p>The proposed method is amazing if we do not assume any prior probabilistic distribution for the variables. Since it relies on empirical distribution, it estimates the class probability for a discrete variable only based on available observations. However, this benefit comes at a (potential) cost. A biased sample will significantly affect the empirical probability and thus the score. It may not be reliable in such cases.</p>
<p>It is also possible that this method will lead to combinatorial explosion and thus very small values of the score. When calculating the empirical probabilities, we will typically have small values — less than 0.3 if there are three classes, say. If there are five such variables, the “average” score would be <span class="math inline">\(0.3^5 = 0.00243\)</span>, which is very small.</p>
<p>This limitation has an easy fix. We could easily scale the score by multiplying it by a large <span class="math inline">\(C\)</span> to bring it on the same scale as the rest of the variables. This will ensure that the clustering algorithm doesn’t penalise this variable for a small default value.</p>
</div>
<div id="quick-example" class="section level1">
<h1>Quick Example</h1>
<p>Let me illustrate the method with a small example. Consider the following data with three discrete variables and no continuous variable.</p>
<p><img src="featured.png" /></p>
<p>The variables have different probability distributions. The probability of being a Male is <span class="math inline">\(2/5\)</span>; being a Female is <span class="math inline">\(3/5\)</span>. The probability of the City being New York is 2/5; Shanghai, Boston or New Delhi are all equal to 1/5 each. The probability of the favourite colour being Blue is <span class="math inline">\(2/5\)</span>; Black, White or Red are at <span class="math inline">\(1/3\)</span>. Finally, being an executive is <span class="math inline">\(3/5\)</span>, and the probability of being a non-executive is <span class="math inline">\(2/5\)</span>.</p>
<p>Assuming that all variables are independent of each other, the probability that a person is Male who lives in Shanghai, whose favourite colour of Blue and who is an executive is <span class="math inline">\(2/5 * 1/5 * 2 /5 * 3/5 = 12/625 = 0.0192\)</span>. I call this joint probability an observation’s score. We could repeat the exercise for all the observations, and we will obtain the results presented in the last column of the table.</p>
<p>This continuous measure that I call “Score” can measure dissimilarity between observations. Note that the method doesn’t guarantee a differentiable score. Even observations with which we get precisely the same can differ from one another. However, observations with very different scores would inevitably be different observations. The latter property is more critical when deciding which cluster an observation belongs to.</p>
</div>
<div id="simulations" class="section level1">
<h1>Simulations</h1>
<p>In this section, I will compare the clusters found using three methods: (1) using only continuous variables, (2) using continuous variables and the score, and (3) using Gower’s distance. For the purpose of this simulation, I will use <code>flower</code> data available in <code>cluster</code> package in R.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p><img src="images/Screen%20Shot%202022-05-10%20at%207.34.57%20PM.png" /></p>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<div id="clusters-with-only-continuous-variables" class="section level3">
<h3>Clusters with only continuous variables</h3>
<p>The clusters obtained from the continuous variables seem to have accounted only for V7 in differentiating between the observations. See the figure below for a scatter plot.</p>
<p><img src="images/cluster_1.png" /></p>
</div>
<div id="clusters-with-continuous-variables-and-my-proposed-score" class="section level3">
<h3>Clusters with continuous variables and my proposed “score”</h3>
<p><img src="images/cluster_2-01.png" /></p>
</div>
<div id="clusters-with-gowers-distance" class="section level3">
<h3>Clusters with Gower’s Distance</h3>
<p>The clusters obtained from Gower’s Distance is presented below.</p>
<p><img src="images/cluster_3.png" /></p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>In this short monograph, I presented a new distance metric based on empirical joint probability. With a small simulation on <code>flower</code> data, I showed how effective it is as compared to not using categorical variables at all. I also compared the results with Gower’s distance-based clustering. I found that the results from the three methods do not match exactly. My method shows some improvement over not using categorical variables. However, Gower is a smart guy and his method performs better than my naive method. :)</p>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>For more details, see <a href="https://cran.r-project.org/web/packages/cluster/cluster.pdf%7D.">https://cran.r-project.org/web/packages/cluster/cluster.pdf</a>. This dataset was first published by Struyf, Hubert and Rousseeuw (1996).<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
